# Revisiting Topological Fairness in Graph Neural Networks from a Causal Perspective
## Abstract
Graph Neural Networks (GNNs) have significantly advanced various graph-based analysis tasks such as social network analysis and recommendation systems. However, as data-driven models, GNNs inherit biases from historical data, raising fairness concerns. In graph data, the effect of topological structure on fairness has been an under-explored problem. Most existing studies attribute topological bias to the homophily effect of sensitive attributes, where nodes with the same sensitive attribute are more likely to establish connections. Here, we will rethink topological fairness from a fine-grained perspective and explore how different forms of links influence fairness. To achieve this, we first propose the concept of Topological Fairness and provide a theoretical analysis. Specifically, we categorize neighborhood information into appropriate and inappropriate, considering inappropriate neighborhood information as a confounding factor that introduces bias. Subsequently, we introduce a method named Graph Fairness Differentiation (GFD), which determines the appropriateness of information by evaluating the sign of the derivative. Finally, our experiments on four datasets achieve competitive performance compared to baseline methods, demonstrating the effectiveness of our approach.

## Introduction
Graph Representation Learning (GRL) is one of the fundamental problems in graph analysis, which has achieved successful applications in areas such as social network analysis, recommendation systems, and bioinformatics. The fundamental goal of GRL is to capture the semantic information of nodes and edges by learning their representation within graph structure. This process facilitates more efficient and precise analysis of graph data. Current methodologies in GRL range from matrix factorization and random walk methods to graph neural network techniques. Among these, Graph Neural Networks (GNNs) have demonstrated groundbreaking performance, standing out as the current hotspot and mainstream in GRL. Researchers have proposed numerous classical GNNs, including Graph Convolutional Network (GCN), Graph Attention Networks (GAT), and Graph Sampling and Aggregation (GraphSAGE), among others. These models have become the primary framework for learning graph representations, which are utilized to address a diverse range of downstream tasks. However, due to the data-driven nature of GNNs, biases and inequalities may inevitably arise during model training. These biases primarily stem from unequal treatment in historical data, societal biases, sampling biases, etc. Such biases can have long-term effects on the model's training, decision-making, and prediction process, potentially leading to unfair decision for specific demographic groups.

To address fairness challenges, researchers have proposed a series of classic fairness concepts, primarily including Group Fairness and Individual Fairness. In the field of GNNs, significant progress has been made to address fairness issues, which can mainly be categorized into three types: adversarial debiasing, fairness constraints, and other innovative debiasing strategies(such as Bayesian-based methods and data preprocessing-based methods). However, how to rationally quantify and eliminate the effect of complex graph structures on fairness remains an urgent challenge. Currently, the homophily effect of sensitive attributes is widely regarded as the primary cause of topological bias, where nodes with the same sensitive attribute are more likely to form connections. Here, we aim to discuss topological bias from a finer-grained perspective, and explore how each different form of edge influences fairness, as shown in Figure \ref{framework1}.

To achieve this goal, we first introduce the concept of Topological Fairness and present a theoretical analysis. It is important to note that, unlike Structural Fairness, which primarily addresses long-tail effects, Topological Fairness focuses on exploring the unbiased nature of topological structures. Specifically, we categorize neighborhood information into appropriate information that benefits fairness predictions and inappropriate information that undermines fairness, where the latter is considered a confounding factor that amplifies bias. Subsequently, to effectively differentiate and intervene in inappropriate information, we introduce the Graph Fairness Differentiation (GFD) method. By achieving a minimum value of GFD within its domain, we can exclude all inappropriate information and identify the optimal fairness graph. The overall conceptual framework of our work is illustrated in Figure \ref{framework}. Finally, we conduct experiments on four datasets, and the results show competitive performance, validating the effectiveness of our proposed method. Our contributions can be summarized as follows:

After rethinking topological bias from a fine-grained perspective, we introduce the concept of topological fairness and present a theoretical analysis.
To differentiate between appropriate and inappropriate information and to intervene in the latter, we propose the GFD method, which aims to eliminate the effects of confounding factors and to determine the optimal fairness graph.
We conduct experiments on four datasets, and the results demonstrate that our method achieves a competitive fairness-utility trade-off, validating the effectiveness of the proposed approach.

## Conclusion

In this paper, we revisit topological bias in GNNs from a fine-grained perspective, with a focus on the influence of each edge on fairness. To achieve this, we first introduce the concept of Topological Fairness, which differentiates neighborhood information into appropriate and inappropriate information, and considers the latter as a confounding factor that introduces bias. Subsequently, to effectively differentiate between these two types of information and intervene in inappropriate information, we propose a novel method called GFD. By evaluating the sign of the derivative, we can determine whether the information is appropriate or inappropriate. By minimizing this value globally, we can mitigate topological bias and identify the optimal fairness graph. Our experiments on four widely used datasets achieve an excellent fairness-utility trade-off, demonstrating the effectiveness and competitiveness of our method. Overall, the definition of Topological Fairness lays the foundation for future research, and applying causal models to examine topological bias has the potential to open up the scope of research in this area.
## Implementation Details
The implementation is based on PyTorch. Specifically, we replicate each experiment using three different random seeds: 1, 10, and 100. All GNNs are optimized with Adam optimizer. The training is conducted for 1,000 epochs with learning rate chosen from \{0.1, 0.01, 0.001\}. The GNNs feature a hidden dimension of 16 and employ dropout rates selected from \{0.2, 0.5, 0.8\}. Additionally, we experiment with iteration numbers for Hessian matrix inverse estimation ranging from \{10, 50, 100, 500, 1000, 5000\} to optimize performance. All experiments are conducted on a device equipped with two GeForce RTX 4080 Super GPUs.
## Requirements
- **Python**: == 3.8.18
- **torch**: == 1.8.0 + cu111
- **cuda**: == 11.1
- **torch-geometric**: == 1.7.0
- **torch-scatter**: == 2.0.6
- **torch-sparse**: == 0.6.9
- **torch-spline-conv**: == 1.2.1
- **dgl**: == 0.6.1
- **scikit-learn**: == 0.23.1
- **numpy**: == 1.22.0
- **scipy**: == 1.4.1
- **networkx**: == 2.4
